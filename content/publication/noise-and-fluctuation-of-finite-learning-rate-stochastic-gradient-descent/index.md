---
title: Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent
subtitle: ""
publication_types:
  - "1"
authors:
  - Kangqiao Liu
  - Liu Ziyin
  - and Masahito Ueda
author_notes:
  - equal contribution
  - equal contribution
doi: http://proceedings.mlr.press/v139/liu21ad.html
publication: Proceedings of the 38th International Conference on Machine
  Learning, PMLR 139:7045-7056, 2021.
publication_short: ICML 2021
abstract: In the vanishing learning rate regime, stochastic gradient descent
  (SGD) is now relatively well understood. In this work, we propose to study the
  basic properties of SGD and its variants in the non-vanishing learning rate
  regime. The focus is on deriving exactly solvable results and discussing their
  implications. The main contributions of this work are to derive the stationary
  distribution for discrete-time SGD in a quadratic loss function with and
  without momentum; in particular, one implication of our result is that the
  fluctuation caused by discrete-time dynamics takes a distorted shape and is
  dramatically larger than a continuous-time theory could predict. Examples of
  applications of the proposed theory considered in this work include the
  approximation error of variants of SGD, the effect of minibatch noise, the
  optimal Bayesian inference, the escape rate from a sharp minimum, and the
  stationary covariance of a few second-order methods including damped Newtonâ€™s
  method, natural gradient descent, and Adam.
draft: false
featured: true
image:
  filename: ""
  focal_point: ""
  preview_only: false
date: 2021-10-01T07:48:21.038Z
---
